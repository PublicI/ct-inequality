{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = require('fs');\n",
    "dsv = require('d3-dsv');\n",
    "simpleStats = require('simple-statistics');\n",
    "_ = require('lodash');\n",
    "csv = require('csv-parser');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_census = dsv.csvParse(fs.readFileSync('too_big/ss14hct.csv', 'utf8'));\n",
    "// from http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_hct.zip\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// these PUMA IDs represent the Bridgeport-Stamford-Norwalk, CT (Metro) according to geocorr\n",
    "bridgeport_2014 = ct_census.filter(d => +d.PUMA >= 100 && +d.PUMA <= 105);\n",
    "\n",
    "/*\n",
    "\"ADJINC\n",
    "Adjustment factor for income and earnings dollar amounts(6 implied decimal places)\n",
    "1008425 .2014 factor (1.008425)\n",
    "\n",
    "Note: The value of ADJINC inflation-adjusts reported income to 2014 dollars.\n",
    "ADJINC applies to variables FINCP and HINCP in the housing record, and variables\n",
    "INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, and WAGP in the person record. \n",
    "\n",
    "HINCP\n",
    "Household income (past 12 months)\n",
    "bbbbbbbbb .N/A(GQ/vacant)\n",
    "000000000 .No household income\n",
    "-00059999 .Loss of -$59,999 or more\n",
    "-00000001..-00059998 .Loss of $1 to -$59,998\n",
    "000000001 .$1 or Break even\n",
    "000000002..999999999 .Total household income in dollars (Components are rounded)\n",
    "\n",
    "Note: Use ADJINC to adjust HINCP to constant dollars.\"\n",
    "\n",
    "https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt\n",
    "*/\n",
    "\n",
    "bridgeport_2014 = bridgeport_2014.filter(d => d.HINCP !== ' ')\n",
    "bridgeport_2014.forEach(d => d.ADJHH = (d.ADJINC/1000000)*d.HINCP);\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiply (row, income_col, weight_col, puma_col) {\n",
    "    let result = [];\n",
    "    for (let i = 0; i < +row[weight_col]; i++) {\n",
    "        // let resultRow = {};\n",
    "        // resultRow[income_col] = row[income_col];\n",
    "        // resultRow[puma_col] = row[puma_col];\n",
    "        result.push(row[income_col]);\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "/*\n",
    "Make a list of HHINCOME for calculating\n",
    "a quantile where each household's HHINCOME\n",
    "is repeated in the series n times, where\n",
    "n == weight_col\n",
    "*/\n",
    "function explode_weights (rows, income_col, weight_col, puma_col) {\n",
    "    let result = [];\n",
    "    for (let row of rows) {\n",
    "        result = result.concat(multiply(row, income_col, weight_col, puma_col));\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "exploded_bridgeport = explode_weights(bridgeport_2014, 'ADJHH', 'WGTP', 'PUMA');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558969.9774999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleStats.quantile(exploded_bridgeport,0.95); // .map(d => d.ADJHH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallelujah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_16 = dsv.csvParse(fs.readFileSync('hh_16.csv', 'latin1'));\n",
    "mable = dsv.csvParse(fs.readFileSync('geocorr14_2014.csv', 'latin1')).slice(1);\n",
    "\n",
    "top_coded = acs_16.filter(d => !d.HD01_VD06)\n",
    "bottom_coded = acs_16.filter(d => !d.HD01_VD02)\n",
    "\n",
    "top_coded.forEach(d => d.States = d['GEO.display-label'].split(', ')[1].split(' ')[0].split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'900102'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_stpuma(st, puma) {\n",
    "    return st + ('00000'+puma).slice(-5);\n",
    "}\n",
    "make_stpuma(9,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_census.forEach(d => d.stpuma = make_stpuma(d.ST,d.PUMA));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: ' ',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: ' ',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '39326.125',\n",
       "    afact: '0.21 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: '22520',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Florence-Muscle Shoals, AL (Metro)',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '147639',\n",
       "    afact: '0.79 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00200',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Limestone & Madison (Outer) Counties--Huntsville City (Far West & Southwest)',\n",
       "    pop14: '183944.849',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100200' },\n",
       "  { state: '01',\n",
       "    puma12: '00301',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville (North) & Madison (East) Cities',\n",
       "    pop14: '124425.297',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100301' },\n",
       "  { state: '01',\n",
       "    puma12: '00302',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville City (Central & South)',\n",
       "    pop14: '106218.3',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100302' } ]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mable.forEach(d => d.stpuma = make_stpuma(d.state, d.puma12))\n",
    "mable.slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0640354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_pumas(geo_id, geo_corr) {\n",
    "    cbsa = geo_corr.filter(d => +d.cbsa == +geo_id);\n",
    "    if (cbsa.length == 0) {\n",
    "        console.log(geo_id);\n",
    "    }\n",
    "    target = {};\n",
    "    for (let row of cbsa) {\n",
    "        target[row.stpuma] = parseFloat(row.afact); // allocation factor (proportion of PUMA within in the geography)\n",
    "    }\n",
    "    return target;\n",
    "}\n",
    "\n",
    "top_coded.forEach(d => d.PUMAS = get_pumas(d['GEO.id2'], mable));\n",
    "all_pumas = _(top_coded).map('PUMAS').flatMap(_.keys).uniq().value();\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this appears not to be used\n",
    "// state_fips = _(mable).groupBy('stab').mapValues(d => _.maxBy(d,'state')).value();\n",
    "// state_fips.CA.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "axios = require('axios');\n",
    "// import zipfile,requests,io,os\n",
    "\n",
    "function fetch_census(x, direct):\n",
    "    let zip_path = `https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_h${x}.zip`;\n",
    "    r = requests.get(zip_path, stream=True)\n",
    "    \n",
    "    axios.get(zip_path, { responseType: 'stream' })\n",
    "        .then(response => {\n",
    "            response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))\n",
    "        });\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct+'/'\n",
    "    dir_path = \"too_big/\"+insert+\"2016/csv_h\"+x+\"/\"\n",
    "    zip_ref.extractall(dir_path)\n",
    "    os.remove(\"too_big/\"+insert+\"2016/csv_h\"+x+\"/ACS2016_PUMS_README.pdf\")\n",
    "\n",
    "def download_unzip_census(ss_s,direct=None):\n",
    "    ss_s.apply(lambda x: fetch_census(x.lower(),direct))\n",
    "*/\n",
    "\n",
    "function make_path(year, state, direct) {\n",
    "  return `too_big/${direct\n",
    "    ? direct + '/'\n",
    "    : ''}${year}/csv_h${state.toLowerCase()}/ss16h${state.toLowerCase()}.csv`;\n",
    "}\n",
    "\n",
    "function get_census_pums_file(s,is_rel=[],direct=null) {\n",
    "    console.log(s);\n",
    "    let result = [];\n",
    "    return new Promise (function (resolve, reject) {\n",
    "        let file = make_path(2016,s,direct)\n",
    "        \n",
    "        fs.createReadStream(file)\n",
    "            .pipe(csv())\n",
    "            .on('data', d => {\n",
    "                let stpuma = make_stpuma(d.ST,d.PUMA);\n",
    "                if (is_rel.indexOf(stpuma) !== -1) {\n",
    "                    d.stpuma = stpuma;\n",
    "                    result.push(d);\n",
    "                }\n",
    "            })\n",
    "            .on('end', d => resolve(result));\n",
    "    });\n",
    "}\n",
    "\n",
    "\n",
    "async function get_census_pums(ss_s,is_rel=[],direct=null) {\n",
    "    let result = [];\n",
    "    for (s of ss_s) {\n",
    "        result = result.concat(await get_census_pums_file(s,is_rel,direct=null));\n",
    "    }\n",
    "    return result;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_16 = dsv.csvParse(fs.readFileSync('ACS_16_1YR_B01003.csv', 'latin1')); // total population by metro area\n",
    "\n",
    "top_coded = top_coded.filter(d =>\n",
    "    (d['GEO.display-label'].indexOf('Metro') !== -1) && // filter to metro areas only\n",
    "    (d['GEO.display-label'].indexOf(' PR ') === -1) && // this might do nothing?\n",
    "    (+pop_16.find(d2 => d['GEO.id2'] == d2['GEO.id2']).HD01_VD01 >= 500000) // population at or over 500,000\n",
    ")\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'CA', 'CT', 'DC', 'MA', 'MD', 'NH', 'NJ', 'NY', 'PA', 'VA', 'WV' ]\n",
      "CA\n",
      "CT\n",
      "DC\n",
      "MA\n",
      "MD\n",
      "NH\n",
      "NJ\n",
      "NY\n",
      "PA\n",
      "VA\n",
      "WV\n"
     ]
    }
   ],
   "source": [
    "async function pums_dataset (tc, is_rel, states_dir=null) {\n",
    "    s = _(tc).flatMap('States').uniq().sort().value();\n",
    "    console.log(s);\n",
    "    // if not (os.path.isfile(make_path(2016,sls[0],states_dir))):\n",
    "    //    download_unzip_census(s,states_dir)\n",
    "    relevant_census = await get_census_pums(s,is_rel,states_dir);\n",
    "    relevant_census = relevant_census.filter(d => d.HINCP !== '');\n",
    "    // relevant_census.forEach(d => d.ADJHH = (d.ADJINC/1000000)*d.HINCP);\n",
    "    // exploded_census = explode_weights(relevant_census, 'HHINC', 'WGTP', 'stpuma');\n",
    "    return relevant_census;\n",
    "}\n",
    "\n",
    "$$.async();\n",
    "\n",
    "relevant_census = [];\n",
    "\n",
    "(function($$) {\n",
    "    pums_dataset(top_coded, all_pumas)\n",
    "        .then(result => {\n",
    "            relevant_census = result;\n",
    "\n",
    "            $$.done();\n",
    "        });\n",
    "})($$);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for (value of top_coded) {\n",
    "    for (entry of Object.keys(value.PUMAS)) {\n",
    "        x.push({\n",
    "            puma: entry,\n",
    "            afact: +value.PUMAS[entry] // allocation factor (proportion of PUMA within in the geography)\n",
    "        });\n",
    "    }\n",
    "}\n",
    "\n",
    "df = x.slice(1)\n",
    "\n",
    "top_coded.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_(df).groupBy(d => d.afact<1).mapValues(d => d.length).value();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// is this the right way to do areal interpolation?\n",
    "function get_data(puma, percent, census) {\n",
    "    let all_entries = explode_weights(census.filter(d => +d.stpuma == +puma), 'HINCP', 'WGTP', 'stpuma');\n",
    "    return percent < 1\n",
    "        ? simpleStats.sample(all_entries, all_entries.length * percent) // seed?\n",
    "        : all_entries;\n",
    "}\n",
    "    \n",
    "function make_fips_data_dict(df,census) {\n",
    "    let d = {};\n",
    "    for (let row of df) {\n",
    "        let result = [];\n",
    "        for (p of Object.keys(row.PUMAS)) {\n",
    "            data = get_data(p,row.PUMAS[p],census);\n",
    "            if (row['GEO.id2'] === '41860') {\n",
    "                console.log(census.filter(d => +d.stpuma == +row.PUMAS[p]),data.length)\n",
    "            }\n",
    "            result = result.concat(data);\n",
    "        }\n",
    "        d[row['GEO.id2']] = result;\n",
    "    }\n",
    "    return d;\n",
    "}\n",
    "\n",
    "fips_data_dict = make_fips_data_dict(top_coded,relevant_census);\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Object.keys(fips_data_dict).filter(d => fips_data_dict[d].length === 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_census.filter(d => +d.stpuma == 600101).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coded.filter(d => +d['GEO.id2'] == 41860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_quantile(fips,percent,data) {\n",
    "    return simpleStats.quantile(data[fips].map(d => +d),percent);\n",
    "}\n",
    "\n",
    "function adjinc(ss,year) {\n",
    "    let file = make_path(year,ss,null);\n",
    "    let result = dsv.csvParse(fs.readFileSync(file, 'utf8'));\n",
    "    return result[0].ADJINC;\n",
    "}\n",
    "\n",
    "adj_inc = adjinc('ct',2016)\n",
    "\n",
    "top_coded = top_coded.forEach(d => d['95_Percentile'] = get_quantile(d['GEO.id2'],.95,fips_data_dict)*adj_inc/1000000)\n",
    "top_coded = top_coded.forEach(d => d['20_Percentile'] = get_quantile(d['GEO.id2'],.2,fips_data_dict)*adj_inc/1000000)\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "9.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
