{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = require('fs');\n",
    "dsv = require('d3-dsv');\n",
    "simpleStats = require('simple-statistics');\n",
    "_ = require('lodash');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_census = dsv.csvParse(fs.readFileSync('too_big/ss14hct.csv', 'utf8'));\n",
    "// from http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_hct.zip\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// these PUMA IDs represent the Bridgeport-Stamford-Norwalk, CT (Metro) according to geocorr\n",
    "bridgeport_2014 = ct_census.filter(d => +d.PUMA >= 100 && +d.PUMA <= 105);\n",
    "\n",
    "/*\n",
    "\"ADJINC\n",
    "Adjustment factor for income and earnings dollar amounts(6 implied decimal places)\n",
    "1008425 .2014 factor (1.008425)\n",
    "\n",
    "Note: The value of ADJINC inflation-adjusts reported income to 2014 dollars.\n",
    "ADJINC applies to variables FINCP and HINCP in the housing record, and variables\n",
    "INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, and WAGP in the person record. \n",
    "\n",
    "HINCP\n",
    "Household income (past 12 months)\n",
    "bbbbbbbbb .N/A(GQ/vacant)\n",
    "000000000 .No household income\n",
    "-00059999 .Loss of -$59,999 or more\n",
    "-00000001..-00059998 .Loss of $1 to -$59,998\n",
    "000000001 .$1 or Break even\n",
    "000000002..999999999 .Total household income in dollars (Components are rounded)\n",
    "\n",
    "Note: Use ADJINC to adjust HINCP to constant dollars.\"\n",
    "\n",
    "https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt\n",
    "*/\n",
    "\n",
    "bridgeport_2014 = bridgeport_2014.filter(d => d.HINCP !== ' ')\n",
    "bridgeport_2014.forEach(d => d.ADJHH = (d.ADJINC/1000000)*d.HINCP);\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiply (row, income_col, weight_col, puma_col) {\n",
    "    let result = [];\n",
    "    for (let i = 0; i < +row[weight_col]; i++) {\n",
    "        let resultRow = {};\n",
    "        resultRow[income_col] = row[income_col];\n",
    "        resultRow[puma_col] = row[puma_col];\n",
    "        result.push(resultRow);\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "/*\n",
    "Make a list of HHINCOME for calculating\n",
    "a quantile where each household's HHINCOME\n",
    "is repeated in the series n times, where\n",
    "n == weight_col\n",
    "*/\n",
    "function explode_weights (rows, income_col, weight_col, puma_col) {\n",
    "    let result = [];\n",
    "    for (row of rows) {\n",
    "        result = result.concat(multiply(row, income_col, weight_col, puma_col));\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "exploded_bridgeport = explode_weights(bridgeport_2014, 'ADJHH', 'WGTP', 'PUMA');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558969.9774999999"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleStats.quantile(exploded_bridgeport.map(d => d.ADJHH),0.95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallelujah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_16 = dsv.csvParse(fs.readFileSync('hh_16.csv', 'latin1'));\n",
    "mable = dsv.csvParse(fs.readFileSync('geocorr14_2014.csv', 'latin1')).slice(1);\n",
    "\n",
    "top_coded = acs_16.filter(d => !d.HD01_VD06)\n",
    "bottom_coded = acs_16.filter(d => !d.HD01_VD02)\n",
    "\n",
    "top_coded.forEach(d => d.States = d['GEO.display-label'].split(', ')[1].split(' ')[0].split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'900102'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_stpuma(st, puma) {\n",
    "    return st + ('00000'+puma).slice(-5);\n",
    "}\n",
    "make_stpuma(9,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_census.forEach(d => d.stpuma = make_stpuma(d.ST,d.PUMA));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: ' ',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: ' ',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '39326.125',\n",
       "    afact: '0.21 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: '22520',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Florence-Muscle Shoals, AL (Metro)',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '147639',\n",
       "    afact: '0.79 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00200',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Limestone & Madison (Outer) Counties--Huntsville City (Far West & Southwest)',\n",
       "    pop14: '183944.849',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100200' },\n",
       "  { state: '01',\n",
       "    puma12: '00301',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville (North) & Madison (East) Cities',\n",
       "    pop14: '124425.297',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100301' },\n",
       "  { state: '01',\n",
       "    puma12: '00302',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville City (Central & South)',\n",
       "    pop14: '106218.3',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100302' } ]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mable.forEach(d => d.stpuma = make_stpuma(d.state, d.puma12))\n",
    "mable.slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0640354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_pumas(geo_id, geo_corr) {\n",
    "    cbsa = geo_corr.filter(d => +d.cbsa == +geo_id);\n",
    "    if (cbsa.length == 0) {\n",
    "        console.log(geo_id);\n",
    "    }\n",
    "    target = {};\n",
    "    for (row of cbsa) {\n",
    "        target[+row.stpuma] = parseFloat(row.afact);\n",
    "    }\n",
    "    return target;\n",
    "}\n",
    "\n",
    "top_coded.forEach(d => d.PUMAS = get_pumas(d['GEO.id2'], mable));\n",
    "all_pumas = _(top_coded).map('PUMAS').flatMap(_.keys).uniq().value();\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_fips = _(mable).groupBy('stab').mapValues(d => _.maxBy(d,'state')).value();\n",
    "state_fips['CA']['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "axios = require('axios');\n",
    "// import zipfile,requests,io,os\n",
    "\n",
    "function fetch_census(x, direct):\n",
    "    let zip_path = `https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_h${x}.zip`;\n",
    "    r = requests.get(zip_path, stream=True)\n",
    "    \n",
    "    axios.get(zip_path, {\n",
    "        responseType: 'stream'\n",
    "    }).then(response => {\n",
    "        response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))\n",
    "    });\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct+'/'\n",
    "    dir_path = \"too_big/\"+insert+\"2016/csv_h\"+x+\"/\"\n",
    "    zip_ref.extractall(dir_path)\n",
    "    os.remove(\"too_big/\"+insert+\"2016/csv_h\"+x+\"/ACS2016_PUMS_README.pdf\")\n",
    "\n",
    "def download_unzip_census(ss_s,direct=None):\n",
    "    ss_s.apply(lambda x: fetch_census(x.lower(),direct))\n",
    "    \n",
    "def make_path(year,state,direct):\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct +'/'\n",
    "    return (\"too_big/\"+insert+str(year)+\"/csv_h\"+state+\"/ss16h\"+state+\".csv\")\n",
    "    \n",
    "def get_census_pums(ss_s,model,direct=None):\n",
    "    tar = pandas.DataFrame(columns=model.columns)\n",
    "    for index, s in ss_s.iteritems():\n",
    "        print(s)\n",
    "        file = make_path(2016,s.lower(),direct)\n",
    "        new = pandas.read_csv(file) ## Grab for every topcoded state\n",
    "        statefip = state_fips.loc[s.upper()][\"state\"]\n",
    "        new[\"stpuma\"] = new.apply(lambda x: make_stpuma(x[\"ST\"],x[\"PUMA\"]),axis=1)\n",
    "        tar = pandas.concat([tar,new])\n",
    "    return tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_16 = dsv.csvParse(fs.readFileSync('ACS_16_1YR_B01003.csv', 'latin1')); // total population by metro area\n",
    "\n",
    "top_coded = top_coded.filter(d =>\n",
    "    (d['GEO.display-label'].indexOf('Metro') !== -1) && // filter to metro areas only\n",
    "    (d['GEO.display-label'].indexOf(' PR ') === -1) && // this might do nothing?\n",
    "    (+pop_16.find(d2 => d['GEO.id2'] == d2['GEO.id2']).HD01_VD01 >= 500000) // population at or over 500,000\n",
    ")\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'MA',\n",
      "  'NH',\n",
      "  'CT',\n",
      "  'CA',\n",
      "  'NY',\n",
      "  'NJ',\n",
      "  'PA',\n",
      "  'CA',\n",
      "  'CA',\n",
      "  'CA',\n",
      "  'DC',\n",
      "  'VA',\n",
      "  'MD',\n",
      "  'WV' ]\n"
     ]
    }
   ],
   "source": [
    "function pums_dataset (tc, is_rel, states_dir=null) {\n",
    "    s = _(tc).flatMap('States').value(); // .uniq().sort().value();\n",
    "    console.log(s)\n",
    "    if not (os.path.isfile(make_path(2016,sls[0],states_dir))):\n",
    "        download_unzip_census(s,states_dir)\n",
    "    census_pums = get_census_pums(s,ct_census,states_dir)\n",
    "    census_pums[\"relevant\"] = census_pums[\"stpuma\"].apply(lambda x: x in is_rel)\n",
    "    relevant_census = census_pums[census_pums[\"relevant\"]]\n",
    "    relevant_census[\"HHINC\"] = pandas.to_numeric(relevant_census[\"HINCP\"],errors=\"coerce\")\n",
    "    exploded_census = explode_weights(relevant_census,\"HHINC\",\"WGTP\",\"stpuma\")\n",
    "    write_obj(exploded_census,exploded_pickle)\n",
    "    return exploded_census*/\n",
    "}\n",
    "    \n",
    "exploded_census = pums_dataset(top_coded, all_pumas)\n",
    "// exploded_census = pandas.DataFrame(exploded_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "9.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
