{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = require('fs');\n",
    "dsv = require('d3-dsv');\n",
    "simpleStats = require('simple-statistics');\n",
    "_ = require('lodash');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_census = dsv.csvParse(fs.readFileSync('too_big/ss14hct.csv', 'utf8'));\n",
    "// from http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_hct.zip\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// these PUMA IDs represent the Bridgeport-Stamford-Norwalk, CT (Metro) according to geocorr\n",
    "bridgeport_2014 = ct_census.filter(d => +d.PUMA >= 100 && +d.PUMA <= 105);\n",
    "\n",
    "/*\n",
    "\"ADJINC\n",
    "Adjustment factor for income and earnings dollar amounts(6 implied decimal places)\n",
    "1008425 .2014 factor (1.008425)\n",
    "\n",
    "Note: The value of ADJINC inflation-adjusts reported income to 2014 dollars.\n",
    "ADJINC applies to variables FINCP and HINCP in the housing record, and variables\n",
    "INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, and WAGP in the person record. \n",
    "\n",
    "HINCP\n",
    "Household income (past 12 months)\n",
    "bbbbbbbbb .N/A(GQ/vacant)\n",
    "000000000 .No household income\n",
    "-00059999 .Loss of -$59,999 or more\n",
    "-00000001..-00059998 .Loss of $1 to -$59,998\n",
    "000000001 .$1 or Break even\n",
    "000000002..999999999 .Total household income in dollars (Components are rounded)\n",
    "\n",
    "Note: Use ADJINC to adjust HINCP to constant dollars.\"\n",
    "\n",
    "https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt\n",
    "*/\n",
    "\n",
    "bridgeport_2014 = bridgeport_2014.filter(d => d.HINCP !== ' ')\n",
    "bridgeport_2014.forEach(d => d.HINCP == ' ' ? '' : d.ADJHH = (d.ADJINC/1000000)*d.HINCP);\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// is there a way to do this without relying on side effects?\n",
    "function multiply (row, income_col, weight_col, puma_col, tar) {\n",
    "    for (let i = 0; i < +row[weight_col]; i++) {\n",
    "        tar.push(row[income_col]);\n",
    "    }\n",
    "}\n",
    "\n",
    "function explode_weights (df, income_col, weight_col, puma_col) {\n",
    "    /*\n",
    "    Make a list of HHINCOME for calculating\n",
    "    a quantile where each household's HHINCOME\n",
    "    is repeated in the series n times, where\n",
    "    n == weight_col\n",
    "    */\n",
    "    tar = []\n",
    "    df.forEach(d => multiply(d,income_col,weight_col,puma_col,tar));\n",
    "    return tar;\n",
    "}\n",
    "\n",
    "exploded_bridgeport = explode_weights(bridgeport_2014,\"ADJHH\",\"WGTP\",\"PUMA\");\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558969.9774999999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleStats.quantile(exploded_bridgeport,0.95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallelujah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ [ ' California' ],\n",
       "  [ ' MA', 'NH Metro Area' ],\n",
       "  [ ' CO Metro Area' ],\n",
       "  [ ' CT Metro Area' ],\n",
       "  [ ' CO Micro Area' ],\n",
       "  [ ' WY', 'ID Micro Area' ],\n",
       "  [ ' NM Micro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' TX Metro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' FL Metro Area' ],\n",
       "  [ ' NY', 'NJ', 'PA Metro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' CA Metro Area' ],\n",
       "  [ ' UT Micro Area' ],\n",
       "  [ ' NJ Metro Area' ],\n",
       "  [ ' MA Micro Area' ],\n",
       "  [ ' DC', 'VA', 'MD', 'WV Metro Area' ],\n",
       "  [ ' ND Micro Area' ] ]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs_16 = dsv.csvParse(fs.readFileSync('hh_16.csv', 'latin1'));\n",
    "mable = dsv.csvParse(fs.readFileSync('geocorr14_2014.csv', 'latin1')).slice(1);\n",
    "// is the mismatch between vintages here a problem?\n",
    "\n",
    "top_coded = acs_16.filter(d => !d.HD01_VD06)\n",
    "bottom_coded = acs_16.filter(d => !d.HD01_VD02)\n",
    "\n",
    "top_coded.forEach(d => d.States = d['GEO.display-label'].split(',')[1].split()[0].split('-'))\n",
    "// not sure this is doing what we want it to\n",
    "top_coded.map(d => d.States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'900102'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_stpuma(st, puma) {\n",
    "    return st + ('00000'+puma).slice(-5);\n",
    "}\n",
    "make_stpuma(9,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_census.forEach(d => d.stpuma = make_stpuma(d.ST,d.PUMA));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: ' ',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: ' ',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '39326.125',\n",
       "    afact: '0.21 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00100',\n",
       "    cbsa: '22520',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Florence-Muscle Shoals, AL (Metro)',\n",
       "    PUMAname: 'Lauderdale, Colbert, Franklin & Marion (Northeast) Counties',\n",
       "    pop14: '147639',\n",
       "    afact: '0.79 ',\n",
       "    stpuma: '0100100' },\n",
       "  { state: '01',\n",
       "    puma12: '00200',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Limestone & Madison (Outer) Counties--Huntsville City (Far West & Southwest)',\n",
       "    pop14: '183944.849',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100200' },\n",
       "  { state: '01',\n",
       "    puma12: '00301',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville (North) & Madison (East) Cities',\n",
       "    pop14: '124425.297',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100301' },\n",
       "  { state: '01',\n",
       "    puma12: '00302',\n",
       "    cbsa: '26620',\n",
       "    stab: 'AL',\n",
       "    cbsaname15: 'Huntsville, AL (Metro)',\n",
       "    PUMAname: 'Huntsville City (Central & South)',\n",
       "    pop14: '106218.3',\n",
       "    afact: '1 ',\n",
       "    stpuma: '0100302' } ]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mable.forEach(d => d.stpuma = make_stpuma(d.state, d.puma12))\n",
    "mable.slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0640354\n"
     ]
    }
   ],
   "source": [
    "all_pumas = [];\n",
    "\n",
    "function get_pumas(geo_id, geo_corr, rel) {\n",
    "    cbsa = geo_corr.filter(d => +d.cbsa == +geo_id);\n",
    "    if (cbsa.length == 0) {\n",
    "        console.log(geo_id);\n",
    "    }\n",
    "    target = {};\n",
    "    for (index in cbsa) {\n",
    "        row = cbsa[index];\n",
    "        \n",
    "        target[+row.stpuma] = parseFloat(row.afact);\n",
    "        rel.push(+row.stpuma);\n",
    "    }\n",
    "    return target;\n",
    "}\n",
    "\n",
    "top_coded.forEach(d => d.PUMAS = get_pumas(d['GEO.id2'], mable, all_pumas));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_fips = _(mable).groupBy('stab').mapValues(d => _.maxBy(d,'state')).value();\n",
    "state_fips['CA']['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "import zipfile,requests,io,os\n",
    "\n",
    "def fetch_census(x,direct):\n",
    "    zip_path = \"https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_h\"+x+\".zip\"\n",
    "    r = requests.get(zip_path, stream=True)\n",
    "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct+'/'\n",
    "    dir_path = \"too_big/\"+insert+\"2016/csv_h\"+x+\"/\"\n",
    "    zip_ref.extractall(dir_path)\n",
    "    os.remove(\"too_big/\"+insert+\"2016/csv_h\"+x+\"/ACS2016_PUMS_README.pdf\")\n",
    "\n",
    "def download_unzip_census(ss_s,direct=None):\n",
    "    ss_s.apply(lambda x: fetch_census(x.lower(),direct))\n",
    "    \n",
    "def make_path(year,state,direct):\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct +'/'\n",
    "    return (\"too_big/\"+insert+str(year)+\"/csv_h\"+state+\"/ss16h\"+state+\".csv\")\n",
    "    \n",
    "def get_census_pums(ss_s,model,direct=None):\n",
    "    tar = pandas.DataFrame(columns=model.columns)\n",
    "    for index, s in ss_s.iteritems():\n",
    "        print(s)\n",
    "        file = make_path(2016,s.lower(),direct)\n",
    "        new = pandas.read_csv(file) ## Grab for every topcoded state\n",
    "        statefip = state_fips.loc[s.upper()][\"state\"]\n",
    "        new[\"stpuma\"] = new.apply(lambda x: make_stpuma(x[\"ST\"],x[\"PUMA\"]),axis=1)\n",
    "        tar = pandas.concat([tar,new])\n",
    "    return tar\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_16 = dsv.csvParse(fs.readFileSync('ACS_16_1YR_B01003.csv', 'latin1')); // total population by metro area\n",
    "\n",
    "top_coded = top_coded.filter(d =>\n",
    "    (d['GEO.display-label'].indexOf('Metro') !== -1) && // filter to metro areas only\n",
    "    (d['GEO.display-label'].indexOf(' PR ') !== -1) && // this might do nothing?\n",
    "    (+pop_16.find(d2 => d['GEO.id2'] == d2['GEO.id2']).HD01_VD01 >= 500000) // population at or over 500,000\n",
    ")\n",
    "\n",
    "top_coded.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Unexpected identifier",
     "execution_count": 45,
     "output_type": "error",
     "traceback": [
      "evalmachine.<anonymous>:6",
      "    if not (os.path.isfile(make_path(2016,sls[0],states_dir))):",
      "       ^^^",
      "",
      "SyntaxError: Unexpected identifier",
      "    at new Script (vm.js:51:7)",
      "    at createScript (vm.js:138:10)",
      "    at Object.runInThisContext (vm.js:199:10)",
      "    at run ([eval]:1002:15)",
      "    at onRunRequest ([eval]:829:18)",
      "    at onMessage ([eval]:789:13)",
      "    at process.emit (events.js:127:13)",
      "    at emit (internal/child_process.js:780:12)",
      "    at process._tickCallback (internal/process/next_tick.js:152:19)"
     ]
    }
   ],
   "source": [
    "function pums_dataset (tc, exploded_pickle, is_rel, states_dir=null) {\n",
    "    s = pandas.Series(tc[\"States\"].sum()).drop_duplicates().sort_values()\n",
    "    s = s[s!=\"California\"]\n",
    "    sls = s.tolist()\n",
    "    print(s)\n",
    "    if not (os.path.isfile(make_path(2016,sls[0],states_dir))):\n",
    "        download_unzip_census(s,states_dir)\n",
    "    census_pums = get_census_pums(s,ct_census,states_dir)\n",
    "    census_pums[\"relevant\"] = census_pums[\"stpuma\"].apply(lambda x: x in is_rel)\n",
    "    relevant_census = census_pums[census_pums[\"relevant\"]]\n",
    "    relevant_census[\"HHINC\"] = pandas.to_numeric(relevant_census[\"HINCP\"],errors=\"coerce\")\n",
    "    exploded_census = explode_weights(relevant_census,\"HHINC\",\"WGTP\",\"stpuma\")\n",
    "    write_obj(exploded_census,exploded_pickle)\n",
    "    return exploded_census\n",
    "}\n",
    "    \n",
    "exploded_census = pums_dataset(top_coded,\"too_big/exploded_census.pickle\",all_pumas)\n",
    "exploded_census = pandas.DataFrame(exploded_census)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "9.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
