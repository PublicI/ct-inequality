{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = require('fs');\n",
    "dsv = require('d3-dsv');\n",
    "simpleStats = require('simple-statistics');\n",
    "_ = require('lodash');\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_census = dsv.csvParse(fs.readFileSync('too_big/ss14hct.csv', 'utf8'));\n",
    "// from http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_hct.zip\n",
    "// let pums = dsv.csvParse(fs.readFileSync('too_big/usa_00002.csv', 'utf8'));\n",
    "// let acs_14 = dsv.csvParse(fs.readFileSync('hh_14.csv', 'latin1'));\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// these PUMA IDs represent the Bridgeport-Stamford-Norwalk, CT (Metro) according to geocorr\n",
    "bridgeport_2014 = ct_census.filter(d => +d.PUMA >= 100 && +d.PUMA <= 105);\n",
    "\n",
    "/*\n",
    "\"ADJINC\n",
    "Adjustment factor for income and earnings dollar amounts(6 implied decimal places)\n",
    "1008425 .2014 factor (1.008425)\n",
    "\n",
    "Note: The value of ADJINC inflation-adjusts reported income to 2014 dollars.\n",
    "ADJINC applies to variables FINCP and HINCP in the housing record, and variables\n",
    "INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, and WAGP in the person record. \n",
    "\n",
    "HINCP\n",
    "Household income (past 12 months)\n",
    "bbbbbbbbb .N/A(GQ/vacant)\n",
    "000000000 .No household income\n",
    "-00059999 .Loss of -$59,999 or more\n",
    "-00000001..-00059998 .Loss of $1 to -$59,998\n",
    "000000001 .$1 or Break even\n",
    "000000002..999999999 .Total household income in dollars (Components are rounded)\n",
    "\n",
    "Note: Use ADJINC to adjust HINCP to constant dollars.\"\n",
    "\n",
    "https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt\n",
    "*/\n",
    "\n",
    "bridgeport_2014 = bridgeport_2014.filter(d => d.HINCP !== ' ')\n",
    "bridgeport_2014.forEach(d => d.HINCP == ' ' ? '' : d.ADJHH = (d.ADJINC/1000000)*d.HINCP);\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiply (row, income_col, weight_col, puma_col, tar) {\n",
    "    for (let i = 0; i < +row[weight_col]; i++) {\n",
    "        tar.push(row[income_col]);\n",
    "    }\n",
    "}\n",
    "\n",
    "function explode_weights (df, income_col, weight_col, puma_col) {\n",
    "    /*\n",
    "    Make a list of HHINCOME for calculating\n",
    "    a quantile where each household's HHINCOME\n",
    "    is repeated in the series n times, where\n",
    "    n == weight_col\n",
    "    */\n",
    "    tar = []\n",
    "    df.forEach(d => multiply(d,income_col,weight_col,puma_col,tar));\n",
    "    return tar;\n",
    "}\n",
    "\n",
    "exploded_bridgeport = explode_weights(bridgeport_2014,\"ADJHH\",\"WGTP\",\"PUMA\");\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558969.9774999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleStats.quantile(exploded_bridgeport,0.95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallelujah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_16 = dsv.csvParse(fs.readFileSync('hh_16.csv', 'latin1'));\n",
    "mable = dsv.csvParse(fs.readFileSync('geocorr14_2014.csv', 'latin1'));\n",
    "\n",
    "top_coded = acs_16.filter(d => !d.HD01_VD06)\n",
    "bottom_coded = acs_16.filter(d => !d.HD01_VD02)\n",
    "\n",
    "top_coded.forEach(d => d['States'] = d['GEO.display-label'].split(',')[1].split()[0].split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'900102'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_stpuma(st, puma) {\n",
    "    return st + ('00000'+puma).slice(-5);\n",
    "}\n",
    "make_stpuma(9,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_census.forEach(d => d.stpuma = make_stpuma(d.ST,d.PUMA));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ { state: '56',\n",
       "    puma12: '00400',\n",
       "    cbsa: '16220',\n",
       "    stab: 'WY',\n",
       "    cbsaname15: 'Casper, WY (Metro)',\n",
       "    PUMAname: 'Natrona, Carbon & Converse Counties',\n",
       "    pop14: '81624',\n",
       "    afact: '0.732 ',\n",
       "    stpuma: '5600400' },\n",
       "  { state: '56',\n",
       "    puma12: '00500',\n",
       "    cbsa: ' ',\n",
       "    stab: 'WY',\n",
       "    cbsaname15: ' ',\n",
       "    PUMAname: 'Sweetwater, Fremont, Uinta, Sublette & Hot Springs Counties--Wind River Reservation',\n",
       "    pop14: '14873',\n",
       "    afact: '0.122 ',\n",
       "    stpuma: '5600500' },\n",
       "  { state: '56',\n",
       "    puma12: '00500',\n",
       "    cbsa: '21740',\n",
       "    stab: 'WY',\n",
       "    cbsaname15: 'Evanston, WY (Micro)',\n",
       "    PUMAname: 'Sweetwater, Fremont, Uinta, Sublette & Hot Springs Counties--Wind River Reservation',\n",
       "    pop14: '20904',\n",
       "    afact: '0.172 ',\n",
       "    stpuma: '5600500' },\n",
       "  { state: '56',\n",
       "    puma12: '00500',\n",
       "    cbsa: '40180',\n",
       "    stab: 'WY',\n",
       "    cbsaname15: 'Riverton, WY (Micro)',\n",
       "    PUMAname: 'Sweetwater, Fremont, Uinta, Sublette & Hot Springs Counties--Wind River Reservation',\n",
       "    pop14: '40703',\n",
       "    afact: '0.335 ',\n",
       "    stpuma: '5600500' },\n",
       "  { state: '56',\n",
       "    puma12: '00500',\n",
       "    cbsa: '40540',\n",
       "    stab: 'WY',\n",
       "    cbsaname15: 'Rock Springs, WY (Micro)',\n",
       "    PUMAname: 'Sweetwater, Fremont, Uinta, Sublette & Hot Springs Counties--Wind River Reservation',\n",
       "    pop14: '45010',\n",
       "    afact: '0.37 ',\n",
       "    stpuma: '5600500' } ]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mable.stpuma = mable.forEach(d => d.stpuma = make_stpuma(d.state, d.puma12))\n",
    "mable.slice(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0640354\n"
     ]
    }
   ],
   "source": [
    "all_pumas = [];\n",
    "\n",
    "function get_pumas(geo_id, geo_corr, rel) {\n",
    "    cbsa = geo_corr.filter(d => +d.cbsa == +geo_id);\n",
    "    if (cbsa.length == 0) {\n",
    "        console.log(geo_id);\n",
    "    }\n",
    "    target = {};\n",
    "    for (index in cbsa) {\n",
    "        row = cbsa[index];\n",
    "        \n",
    "        target[+row.stpuma] = parseFloat(row.afact);\n",
    "        rel.push(+row.stpuma);\n",
    "    }\n",
    "    return target;\n",
    "}\n",
    "\n",
    "top_coded.forEach(d => d.PUMAS = get_pumas(d['GEO.id2'], mable, all_pumas));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_fips = _(mable).groupBy('stab').mapValues(d => _.maxBy(d,'state').state).value(); // [['state']].max()\n",
    "state_fips['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "import zipfile,requests,io,os\n",
    "\n",
    "def fetch_census(x,direct):\n",
    "    zip_path = \"https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_h\"+x+\".zip\"\n",
    "    r = requests.get(zip_path, stream=True)\n",
    "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct+'/'\n",
    "    dir_path = \"too_big/\"+insert+\"2016/csv_h\"+x+\"/\"\n",
    "    zip_ref.extractall(dir_path)\n",
    "    os.remove(\"too_big/\"+insert+\"2016/csv_h\"+x+\"/ACS2016_PUMS_README.pdf\")\n",
    "\n",
    "def download_unzip_census(ss_s,direct=None):\n",
    "    ss_s.apply(lambda x: fetch_census(x.lower(),direct))\n",
    "    \n",
    "def make_path(year,state,direct):\n",
    "    insert = \"\"\n",
    "    if direct is not None:\n",
    "        insert = direct +'/'\n",
    "    return (\"too_big/\"+insert+str(year)+\"/csv_h\"+state+\"/ss16h\"+state+\".csv\")\n",
    "    \n",
    "def get_census_pums(ss_s,model,direct=None):\n",
    "    tar = pandas.DataFrame(columns=model.columns)\n",
    "    for index, s in ss_s.iteritems():\n",
    "        print(s)\n",
    "        file = make_path(2016,s.lower(),direct)\n",
    "        new = pandas.read_csv(file) ## Grab for every topcoded state\n",
    "        statefip = state_fips.loc[s.upper()][\"state\"]\n",
    "        new[\"stpuma\"] = new.apply(lambda x: make_stpuma(x[\"ST\"],x[\"PUMA\"]),axis=1)\n",
    "        tar = pandas.concat([tar,new])\n",
    "    return tar\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_16 = dsv.csvParse(fs.readFileSync('ACS_16_1YR_B01003.csv', 'latin1'));\n",
    "top_coded = top_coded.join(pop_16[[\"GEO.id2\",\"HD01_VD01\"]].set_index(\"GEO.id2\"),on=\"GEO.id2\")\n",
    "top_coded[\"us_metro\"] = top_coded.apply(lambda x: (\"Metro\" in x[\"GEO.display-label\"]) & (\" PR \" not in x[\"GEO.display-label\"]) & (x[\"HD01_VD01\"] >= 500000),axis=1)\n",
    "top_coded = top_coded[top_coded[\"us_metro\"]==True]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "9.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
