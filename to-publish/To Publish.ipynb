{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Inequality\n",
    "\n",
    "To get a sense of how inequality in Connecticut's cities compared nationally, we decided to look at the ratio between two benchmarks: The lowest combined income a household could earn while still breaking into the top 5 percent of household income and the highest combined income a household could earn while still falling in the bottom 20 percent. The farther apart these two numbers are, proportionally, the greater the gap between rich and poor.\n",
    "\n",
    "The US Census Bureau provides estimates of the numbers we need in a table titled [HOUSEHOLD INCOME QUINTILE UPPER LIMITS](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_16_5YR_B19080&prodType=table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO.id2</th>\n",
       "      <th>GEO.display-label</th>\n",
       "      <th>Upper_Limit_Bottom_20</th>\n",
       "      <th>Lower_Limit_Top_5</th>\n",
       "      <th>respop72016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35620</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA Metro Area</td>\n",
       "      <td>25717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20275179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31080</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA Metro Area</td>\n",
       "      <td>25626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13328261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16980</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI Metro Area</td>\n",
       "      <td>25921</td>\n",
       "      <td>245949.0</td>\n",
       "      <td>9546326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19100</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX Metro Area</td>\n",
       "      <td>28601</td>\n",
       "      <td>234781.0</td>\n",
       "      <td>7253424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26420</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX Metro Area</td>\n",
       "      <td>25785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6798010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47900</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV M...</td>\n",
       "      <td>41076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6150681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>21198</td>\n",
       "      <td>221668.0</td>\n",
       "      <td>6107433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37980</td>\n",
       "      <td>Philadelphia-Camden-Wilmington, PA-NJ-DE-MD Me...</td>\n",
       "      <td>25571</td>\n",
       "      <td>246971.0</td>\n",
       "      <td>6077152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12060</td>\n",
       "      <td>Atlanta-Sandy Springs-Roswell, GA Metro Area</td>\n",
       "      <td>26684</td>\n",
       "      <td>234699.0</td>\n",
       "      <td>5795723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14460</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH Metro Area</td>\n",
       "      <td>31367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4805942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GEO.id2                                  GEO.display-label  \\\n",
       "0    35620   New York-Newark-Jersey City, NY-NJ-PA Metro Area   \n",
       "1    31080      Los Angeles-Long Beach-Anaheim, CA Metro Area   \n",
       "2    16980      Chicago-Naperville-Elgin, IL-IN-WI Metro Area   \n",
       "3    19100         Dallas-Fort Worth-Arlington, TX Metro Area   \n",
       "4    26420    Houston-The Woodlands-Sugar Land, TX Metro Area   \n",
       "5    47900  Washington-Arlington-Alexandria, DC-VA-MD-WV M...   \n",
       "6    33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "7    37980  Philadelphia-Camden-Wilmington, PA-NJ-DE-MD Me...   \n",
       "8    12060       Atlanta-Sandy Springs-Roswell, GA Metro Area   \n",
       "9    14460          Boston-Cambridge-Newton, MA-NH Metro Area   \n",
       "\n",
       "   Upper_Limit_Bottom_20  Lower_Limit_Top_5  respop72016  \n",
       "0                  25717                NaN     20275179  \n",
       "1                  25626                NaN     13328261  \n",
       "2                  25921           245949.0      9546326  \n",
       "3                  28601           234781.0      7253424  \n",
       "4                  25785                NaN      6798010  \n",
       "5                  41076                NaN      6150681  \n",
       "6                  21198           221668.0      6107433  \n",
       "7                  25571           246971.0      6077152  \n",
       "8                  26684           234699.0      5795723  \n",
       "9                  31367                NaN      4805942  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "acs = pandas.read_csv('ACS_16_1YR_B19080-1/ACS_16_1YR_B19080.csv')\n",
    "\n",
    "## Bottom 20 cutoff is HD01_VD02, top 5 cutoff is HD01_VD06\n",
    "acs = acs[[\"GEO.id2\",\"GEO.display-label\",\"HD01_VD02\",\"HD01_VD06\"]].rename(columns = {\"HD01_VD02\":\"Upper_Limit_Bottom_20\",\"HD01_VD06\":\"Lower_Limit_Top_5\"})\n",
    "\n",
    "## We're only interested in the 100 biggest metros, so let's join the resident population table and\n",
    "## only include the largest.\n",
    "#\n",
    "## Link to census: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=PEP_2017_PEPANNRES&prodType=table\n",
    "#\n",
    "pop_16 = pandas.read_csv(\"PEP_2017_PEPANNRES-1/PEP_2017_PEPANNRES.csv\",encoding = \"ISO-8859-1\")\n",
    "acs = acs.join(pop_16[[\"GEO.id2\",\"respop72016\"]].set_index(\"GEO.id2\"),on=\"GEO.id2\")\n",
    "acs = acs.sort_values(\"respop72016\",ascending=False).reset_index(drop=True)\n",
    "acs = acs.loc[0:99]\n",
    "\n",
    "acs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem!\n",
    "There are values missing from this table! The Census doesn't report estimates higher than $250,000. This is an example of [topcoding](link), which the Census says it does for privacy reasons. 16 of the cities in our dataset have topcoded lower limits to their top 5 percent of income earners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO.id2</th>\n",
       "      <th>GEO.display-label</th>\n",
       "      <th>Upper_Limit_Bottom_20</th>\n",
       "      <th>Lower_Limit_Top_5</th>\n",
       "      <th>respop72016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35620</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA Metro Area</td>\n",
       "      <td>25717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20275179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31080</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA Metro Area</td>\n",
       "      <td>25626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13328261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26420</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX Metro Area</td>\n",
       "      <td>25785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6798010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47900</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV M...</td>\n",
       "      <td>41076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6150681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14460</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH Metro Area</td>\n",
       "      <td>31367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4805942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41860</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA Metro Area</td>\n",
       "      <td>36353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4699077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42660</td>\n",
       "      <td>Seattle-Tacoma-Bellevue, WA Metro Area</td>\n",
       "      <td>34576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3802660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41740</td>\n",
       "      <td>San Diego-Carlsbad, CA Metro Area</td>\n",
       "      <td>30265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19740</td>\n",
       "      <td>Denver-Aurora-Lakewood, CO Metro Area</td>\n",
       "      <td>32297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2851848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12580</td>\n",
       "      <td>Baltimore-Columbia-Towson, MD Metro Area</td>\n",
       "      <td>31209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2801028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12420</td>\n",
       "      <td>Austin-Round Rock, TX Metro Area</td>\n",
       "      <td>31674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2060558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>41940</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA Metro Area</td>\n",
       "      <td>41879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>25540</td>\n",
       "      <td>Hartford-West Hartford-East Hartford, CT Metro...</td>\n",
       "      <td>29378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1210075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>46520</td>\n",
       "      <td>Urban Honolulu, HI Metro Area</td>\n",
       "      <td>36055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>992761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14860</td>\n",
       "      <td>Bridgeport-Stamford-Norwalk, CT Metro Area</td>\n",
       "      <td>34119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>949191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>37100</td>\n",
       "      <td>Oxnard-Thousand Oaks-Ventura, CA Metro Area</td>\n",
       "      <td>34721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>851096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GEO.id2                                  GEO.display-label  \\\n",
       "0     35620   New York-Newark-Jersey City, NY-NJ-PA Metro Area   \n",
       "1     31080      Los Angeles-Long Beach-Anaheim, CA Metro Area   \n",
       "4     26420    Houston-The Woodlands-Sugar Land, TX Metro Area   \n",
       "5     47900  Washington-Arlington-Alexandria, DC-VA-MD-WV M...   \n",
       "9     14460          Boston-Cambridge-Newton, MA-NH Metro Area   \n",
       "10    41860       San Francisco-Oakland-Hayward, CA Metro Area   \n",
       "14    42660             Seattle-Tacoma-Bellevue, WA Metro Area   \n",
       "16    41740                  San Diego-Carlsbad, CA Metro Area   \n",
       "18    19740              Denver-Aurora-Lakewood, CO Metro Area   \n",
       "20    12580           Baltimore-Columbia-Towson, MD Metro Area   \n",
       "30    12420                   Austin-Round Rock, TX Metro Area   \n",
       "34    41940      San Jose-Sunnyvale-Santa Clara, CA Metro Area   \n",
       "46    25540  Hartford-West Hartford-East Hartford, CT Metro...   \n",
       "53    46520                      Urban Honolulu, HI Metro Area   \n",
       "56    14860         Bridgeport-Stamford-Norwalk, CT Metro Area   \n",
       "65    37100        Oxnard-Thousand Oaks-Ventura, CA Metro Area   \n",
       "\n",
       "    Upper_Limit_Bottom_20  Lower_Limit_Top_5  respop72016  \n",
       "0                   25717                NaN     20275179  \n",
       "1                   25626                NaN     13328261  \n",
       "4                   25785                NaN      6798010  \n",
       "5                   41076                NaN      6150681  \n",
       "9                   31367                NaN      4805942  \n",
       "10                  36353                NaN      4699077  \n",
       "14                  34576                NaN      3802660  \n",
       "16                  30265                NaN      3317200  \n",
       "18                  32297                NaN      2851848  \n",
       "20                  31209                NaN      2801028  \n",
       "30                  31674                NaN      2060558  \n",
       "34                  41879                NaN      1990910  \n",
       "46                  29378                NaN      1210075  \n",
       "53                  36055                NaN       992761  \n",
       "56                  34119                NaN       949191  \n",
       "65                  34721                NaN       851096  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_coded = acs[acs[\"Lower_Limit_Top_5\"].isnull()]\n",
    "top_coded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PUMS to impute missing census aggregates\n",
    "\n",
    "Luckily, we can use the [Public Use Microdata Sample (PUMS)](https://www.census.gov/programs-surveys/acs/technical-documentation/pums.html) to estimate these values ourselves.\n",
    "\n",
    "[According to the Census](https://www.census.gov/programs-surveys/acs/technical-documentation/pums.html):\n",
    "\n",
    ">*The American Community Survey (ACS) Public Use Microdata Sample (PUMS) files are a set of untabulated records about individual people or housing units. The Census Bureau produces the PUMS files so that data users can create custom tables that are not available through pretabulated (or summary) ACS data products.*\n",
    "\n",
    "PUMS are also topcoded, but they're topcoded at $999,999 [CHECK THIS] which should let us get a good estimate of the lower cut-off of the top 5 percent of households.\n",
    "\n",
    "Getting this PUMS data into a format we can work with takes a long time. Steps 2-4 take an especially long time, so we've cached the results of each of these steps in pickled objects. To create them from scratch, delete or rename the pickles spsecified at the beginning of each step.\n",
    "\n",
    "### 1. Correlate PUMAs to CBSAs\n",
    "\n",
    "In order to link the metro areas to the PUMS data, we need to map the GEO.id2 (FIPs codes) to the only geographic type provided in the PUMS data: Public Use Microdata Areas(PUMA). To do this we use the Missouri Census Data Center [Geographic Correspondence Engine](http://mcdc.missouri.edu/websas/geocorr14.html). Let's pull the mappings for Core Based Statistical Areas (CBSA) in all states, because we'll want them later:\n",
    "\n",
    "![shot1](Screenshots/all_states.png)\n",
    "\n",
    "We'll be mapping PUMAs to CBSAs (MSAs are a kind of CBSA):\n",
    "\n",
    "![shot2](Screenshots/mapping.png)\n",
    "\n",
    "And most PUMAs are entirely within a CBSA, but for those that aren't, Geocorr will estimate the percentage of the PUMA's population contained within that CBSA based off of 2014 measurements:\n",
    "\n",
    "![shot3](Screenshots/population.png)\n",
    "\n",
    "Give it a moment to build the file. Then download the CSV, make sure it's named \"geocorr14.csv\", and put it in the same directory as this notebook. And then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>puma12</th>\n",
       "      <th>cbsa</th>\n",
       "      <th>stab</th>\n",
       "      <th>cbsaname15</th>\n",
       "      <th>PUMAname</th>\n",
       "      <th>pop14</th>\n",
       "      <th>afact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>00100</td>\n",
       "      <td></td>\n",
       "      <td>AL</td>\n",
       "      <td></td>\n",
       "      <td>Lauderdale, Colbert, Franklin &amp; Marion (Northe...</td>\n",
       "      <td>39326.125</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>00100</td>\n",
       "      <td>22520</td>\n",
       "      <td>AL</td>\n",
       "      <td>Florence-Muscle Shoals, AL (Metro)</td>\n",
       "      <td>Lauderdale, Colbert, Franklin &amp; Marion (Northe...</td>\n",
       "      <td>147639</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>00200</td>\n",
       "      <td>26620</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Limestone &amp; Madison (Outer) Counties--Huntsvil...</td>\n",
       "      <td>183944.849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>00301</td>\n",
       "      <td>26620</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Huntsville (North) &amp; Madison (East) Cities</td>\n",
       "      <td>124425.297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01</td>\n",
       "      <td>00302</td>\n",
       "      <td>26620</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Huntsville City (Central &amp; South)</td>\n",
       "      <td>106218.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state puma12   cbsa stab                          cbsaname15  \\\n",
       "1    01  00100          AL                                       \n",
       "2    01  00100  22520   AL  Florence-Muscle Shoals, AL (Metro)   \n",
       "3    01  00200  26620   AL              Huntsville, AL (Metro)   \n",
       "4    01  00301  26620   AL              Huntsville, AL (Metro)   \n",
       "5    01  00302  26620   AL              Huntsville, AL (Metro)   \n",
       "\n",
       "                                            PUMAname       pop14  afact  \n",
       "1  Lauderdale, Colbert, Franklin & Marion (Northe...   39326.125  0.21   \n",
       "2  Lauderdale, Colbert, Franklin & Marion (Northe...      147639  0.79   \n",
       "3  Limestone & Madison (Outer) Counties--Huntsvil...  183944.849     1   \n",
       "4         Huntsville (North) & Madison (East) Cities  124425.297     1   \n",
       "5                  Huntsville City (Central & South)    106218.3     1   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...open it up!\n",
    "geocorr = pandas.read_csv(\"geocorr14.csv\", encoding = \"ISO-8859-1\")[1:]\n",
    "geocorr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUMAs are unique within states but not between them, so to map them to nationally unique FIPs codes (the 'cbsa' column) we'll need to combine the 'state' and 'puma12' columns into a new field like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>puma12</th>\n",
       "      <th>cbsa</th>\n",
       "      <th>stab</th>\n",
       "      <th>cbsaname15</th>\n",
       "      <th>PUMAname</th>\n",
       "      <th>pop14</th>\n",
       "      <th>afact</th>\n",
       "      <th>stpuma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>00100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AL</td>\n",
       "      <td></td>\n",
       "      <td>Lauderdale, Colbert, Franklin &amp; Marion (Northe...</td>\n",
       "      <td>39326.125</td>\n",
       "      <td>0.21</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>00100</td>\n",
       "      <td>22520.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Florence-Muscle Shoals, AL (Metro)</td>\n",
       "      <td>Lauderdale, Colbert, Franklin &amp; Marion (Northe...</td>\n",
       "      <td>147639</td>\n",
       "      <td>0.79</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>00200</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Limestone &amp; Madison (Outer) Counties--Huntsvil...</td>\n",
       "      <td>183944.849</td>\n",
       "      <td>1</td>\n",
       "      <td>100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>00301</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Huntsville (North) &amp; Madison (East) Cities</td>\n",
       "      <td>124425.297</td>\n",
       "      <td>1</td>\n",
       "      <td>100301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01</td>\n",
       "      <td>00302</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Huntsville, AL (Metro)</td>\n",
       "      <td>Huntsville City (Central &amp; South)</td>\n",
       "      <td>106218.3</td>\n",
       "      <td>1</td>\n",
       "      <td>100302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state puma12     cbsa stab                          cbsaname15  \\\n",
       "1    01  00100      NaN   AL                                       \n",
       "2    01  00100  22520.0   AL  Florence-Muscle Shoals, AL (Metro)   \n",
       "3    01  00200  26620.0   AL              Huntsville, AL (Metro)   \n",
       "4    01  00301  26620.0   AL              Huntsville, AL (Metro)   \n",
       "5    01  00302  26620.0   AL              Huntsville, AL (Metro)   \n",
       "\n",
       "                                            PUMAname       pop14  afact  \\\n",
       "1  Lauderdale, Colbert, Franklin & Marion (Northe...   39326.125  0.21    \n",
       "2  Lauderdale, Colbert, Franklin & Marion (Northe...      147639  0.79    \n",
       "3  Limestone & Madison (Outer) Counties--Huntsvil...  183944.849     1    \n",
       "4         Huntsville (North) & Madison (East) Cities  124425.297     1    \n",
       "5                  Huntsville City (Central & South)    106218.3     1    \n",
       "\n",
       "   stpuma  \n",
       "1  100100  \n",
       "2  100100  \n",
       "3  100200  \n",
       "4  100301  \n",
       "5  100302  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_stpuma(st,puma):\n",
    "    return int(str(st)+str(puma).rjust(5,'0'))\n",
    "\n",
    "geocorr['state'] = geocorr.\n",
    "geocorr['stpuma'] = geocorr.apply(lambda x: make_stpuma(x[\"state\"],x[\"puma12\"]),axis=1)\n",
    "geocorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "geocorr[\"cbsa\"] = geocorr[\"cbsa\"].apply(lambda x : pandas.to_numeric(x,errors=\"coerce\"))\n",
    "\n",
    "#\n",
    "## Go through the topcoded CBSAs and make a dict for each CBSA. The keys\n",
    "## are the codes of stpumas that overlap with the CBSA, and the values are \n",
    "## Geocorr's estimate of the percentage of the PUMA's population inside \n",
    "## the CBSA. E.g. {stpuma: afact} -- we'll need these later\n",
    "#\n",
    "def get_pumas(geo_id,geo_corr):\n",
    "    cbsa = geo_corr[geo_corr[\"cbsa\"] == geo_id] \n",
    "    if len(cbsa) == 0:\n",
    "        print(geo_id)\n",
    "    target = {}\n",
    "    for index,row in cbsa.iterrows():\n",
    "        target[int(row[\"stpuma\"])] = float(row[\"afact\"])\n",
    "    return target\n",
    "\n",
    "top_coded[\"PUMAS\"] = top_coded[\"GEO.id2\"].apply(lambda x: get_pumas(x,geocorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO.id2</th>\n",
       "      <th>GEO.display-label</th>\n",
       "      <th>Upper_Limit_Bottom_20</th>\n",
       "      <th>Lower_Limit_Top_5</th>\n",
       "      <th>respop72016</th>\n",
       "      <th>PUMAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35620</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA Metro Area</td>\n",
       "      <td>25717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20275179</td>\n",
       "      <td>{3400301: 1.0, 3400302: 1.0, 3400303: 1.0, 340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31080</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA Metro Area</td>\n",
       "      <td>25626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13328261</td>\n",
       "      <td>{603701: 1.0, 603702: 1.0, 603703: 1.0, 603704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26420</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX Metro Area</td>\n",
       "      <td>25785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6798010</td>\n",
       "      <td>{4804400: 1.0, 4804501: 1.0, 4804502: 1.0, 480...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47900</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV M...</td>\n",
       "      <td>41076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6150681</td>\n",
       "      <td>{1100101: 1.0, 1100102: 1.0, 1100103: 1.0, 110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14460</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH Metro Area</td>\n",
       "      <td>31367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4805942</td>\n",
       "      <td>{2500400: 0.352, 2500501: 1.0, 2500502: 1.0, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GEO.id2                                  GEO.display-label  \\\n",
       "0    35620   New York-Newark-Jersey City, NY-NJ-PA Metro Area   \n",
       "1    31080      Los Angeles-Long Beach-Anaheim, CA Metro Area   \n",
       "4    26420    Houston-The Woodlands-Sugar Land, TX Metro Area   \n",
       "5    47900  Washington-Arlington-Alexandria, DC-VA-MD-WV M...   \n",
       "9    14460          Boston-Cambridge-Newton, MA-NH Metro Area   \n",
       "\n",
       "   Upper_Limit_Bottom_20  Lower_Limit_Top_5  respop72016  \\\n",
       "0                  25717                NaN     20275179   \n",
       "1                  25626                NaN     13328261   \n",
       "4                  25785                NaN      6798010   \n",
       "5                  41076                NaN      6150681   \n",
       "9                  31367                NaN      4805942   \n",
       "\n",
       "                                               PUMAS  \n",
       "0  {3400301: 1.0, 3400302: 1.0, 3400303: 1.0, 340...  \n",
       "1  {603701: 1.0, 603702: 1.0, 603703: 1.0, 603704...  \n",
       "4  {4804400: 1.0, 4804501: 1.0, 4804502: 1.0, 480...  \n",
       "5  {1100101: 1.0, 1100102: 1.0, 1100103: 1.0, 110...  \n",
       "9  {2500400: 0.352, 2500501: 1.0, 2500502: 1.0, 2...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_coded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fetch and parse PUMS data\n",
    "\n",
    "*This repo has the next few steps of data collection cached, in a file named \"pums_we_want.pickle\". To do the collection from scratch, delete \"pums_we_want.pickle\"*\n",
    "\n",
    "The files we source the PUMS data from are too big for github, but can be downloaded directly from the census:\n",
    "\n",
    "https://www.census.gov/programs-surveys/acs/data/pums.html\n",
    "\n",
    "Our code fetches the files we need, puts all the data in a single DataFrame, zips them up and pickles them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8,  9, 11, 15, 24, 25, 33, 34, 36, 42, 48, 51, 53, 54])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_pumas[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import zipfile,requests,io,os\n",
    "\n",
    "#\n",
    "# This is a table of all the PUMAs we care to look up, and their corresponding\n",
    "# state fips and CBSA codes. It will throw errors, but they can be ignored. \n",
    "#\n",
    "relevant_pumas = geocorr[geocorr[\"cbsa\"].isin(top_coded[\"GEO.id2\"])]\n",
    "\n",
    "#\n",
    "# We need our state and puma codes to be numerical, not strings, in order to\n",
    "# compare them to the codes in the PUMS data.\n",
    "#\n",
    "relevant_pumas[\"state\"] = relevant_pumas[\"state\"].apply(lambda x: pandas.to_numeric(x))\n",
    "relevant_pumas[\"puma12\"] = relevant_pumas[\"puma12\"].apply(lambda x: pandas.to_numeric(x))\n",
    "\n",
    "#\n",
    "# The household Census PUMS for every state in the US are available at this url: \n",
    "# https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_hus.zip\n",
    "# The following code downloads the zip, unzips it, and deletes the irrelevant files.\n",
    "#\n",
    "def download_unzip_census(path,directory):\n",
    "    r = requests.get(path,stream=True)\n",
    "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    zip_ref.extractall(directory) \n",
    "\n",
    "    # Delete any files you don't want\n",
    "    files = [\"ss16husa.csv\",\"ss16husb.csv\"]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename not in files:\n",
    "            os.remove(directory+\"/\"+filename)\n",
    "    # [[ Do I need to return something for style points? ]]\n",
    "    \n",
    "#\n",
    "# Take out all states that aren't in our 'relevant' DataFrame, then take out \n",
    "# all PUMA codes that aren't synonymous with PUMAs in our 'relevant' DataFrame.\n",
    "#\n",
    "# This will leave in some PUMAs in the wrong state, but lets us do our make_stpuma()\n",
    "# concatenation on a much smaller subset of the data, that still includes all the \n",
    "# rows we need. Once we do that concatenation, we can zero in on exactly the \n",
    "# rows corresponding to our desired stpumas.\n",
    "#\n",
    "def filter_geos(f,relevant):\n",
    "    f = f[(f[\"ST\"].isin(relevant[\"state\"].unique()))]\n",
    "    f = f[f[\"PUMA\"].isin(relevant[\"puma12\"].unique())]\n",
    "    f[\"stpuma\"] = f.apply(lambda x: make_stpuma(x[\"ST\"],x[\"PUMA\"]),axis=1)\n",
    "    f = f[f[\"stpuma\"].isin(relevant[\"stpuma\"].unique())]\n",
    "    return f        \n",
    "\n",
    "\n",
    "#\n",
    "# If the census files aren't unzipped locally, do that. Then load the contents of \n",
    "# both csvs into a single DataFrame\n",
    "#\n",
    "def load_pums(directory,relevant):\n",
    "    if (os.path.isfile(\"too_big/pums/ss16husa.csv\") & os.path.isfile(\"too_big/pums/ss16husa.csv\")):\n",
    "        download_unzip_census(\"https://www2.census.gov/programs-surveys/acs/data/pums/2016/1-Year/csv_hus.zip\",\"too_big/pums\")\n",
    "    pums = pandas.DataFrame({})\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename[0]!='.':\n",
    "            tmp = pandas.read_csv(directory+\"/\"+filename)\n",
    "            print(tmp.size)\n",
    "            tmp = filter_geos(tmp,relevant)\n",
    "            print(tmp.size)\n",
    "            pums = pandas.concat([pums,tmp])\n",
    "            print(pums.size)\n",
    "    return pums\n",
    "\n",
    "#\n",
    "# If the pickle exists, fetch that. If not, pull the data again and pickle it. The \n",
    "# repo contains the pickle by default, so to rerun this code, delete the pickle or\n",
    "# or rename it\n",
    "#\n",
    "def pums_data(pickle_name,sourcedir,pumas_we_want):\n",
    "    if (os.path.isfile(pickle_name)):\n",
    "        return pandas.read_pickle(pickle_name,\"gzip\")\n",
    "    target = load_pums(sourcedir,pumas_we_want)\n",
    "    target.to_pickle(pickle_name,\"gzip\")\n",
    "    return target\n",
    " \n",
    "pums_we_want = pums_data(\"pums_we_want.pickle\",\"too_big/pums/\",relevant_pumas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316676, 231)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pums_we_want.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulate weighted data\n",
    "\n",
    "*This step is also cached in a file named \"expanded.pickle\". To do the processing from scratch, delete \"expanded.pickle\".*\n",
    "\n",
    "Now we have all the PUMS data for every geography we care about. This dataframe has a column \"WGTP\" which indicates how many households each particular record represents. In order to get accurate percentiles, we need each row to represent a single household.\n",
    "\n",
    "We're going to convert the data by duplicating each row _n_ times, where _n_ equals the row's WGTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-296-022b25666b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_expanded_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expanded.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_expand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HINCP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WGTP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stpuma\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-296-022b25666b96>\u001b[0m in \u001b[0;36mget_expanded_data\u001b[0;34m(picklename, df, income_col, weight_col, stpuma_col)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplode_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mincome_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstpuma_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-296-022b25666b96>\u001b[0m in \u001b[0;36mexplode_weights\u001b[0;34m(df, income_col, weight_col, stpuma_col)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincome_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstpuma_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# !!!!\n",
    "#\n",
    "# Actually doing this takes hours and I'm not sure if there's a way I could speed it\n",
    "# up. Pretty serious obstacle to reproducibility.\n",
    "#\n",
    "# I could hit it state by state and write those pickles separately. :shrug: This will\n",
    "# be too big for github anyways, it won't in individual state files\n",
    "#\n",
    "# Oh but accessing by state will be a pain in the butt later. Maybe I'll store them \n",
    "# in CBSA files. UHHH BRAIN MELTING.\n",
    "#\n",
    "# !!!!\n",
    "#\n",
    "# Wait, this might be taking so long because it is broken. Or because iterrows is just\n",
    "# way slower than apply\n",
    "\n",
    "# Our PUMS DataFrame has a LOT in it that we don't need,\n",
    "# so let's shrink it down before processing it\n",
    "\n",
    "columns_we_want = [\"HINCP\",\"WGTP\", \"stpuma\"]\n",
    "to_expand = pums_we_want[columns_we_want]\n",
    "\n",
    "# This isn't holding for some reason, keeps reverting to float\n",
    "#to_expand[\"WGTP\"] = to_expand[\"WGTP\"].astype(int) \n",
    "\n",
    "\n",
    "# Make a list of length row[weight_col] where each entry is a dict: {\"stpuma\":stpuma,\"HINCP\":income}\n",
    "def multiply(row, income_col, weight_col, stpuma_col):\n",
    "    return [{\n",
    "        stpuma_col: row[stpuma_col],\n",
    "        income_col: row[income_col]\n",
    "    } for i in range(int(row[weight_col]))]\n",
    "\n",
    "# Make a dataframe where each row in our PUMS data is repeated for n rows, where n == row[weight_col]\n",
    "def explode_weights(df, income_col, weight_col, stpuma_col):\n",
    "    i =0\n",
    "    result = []\n",
    "    for index, row in df.iterrows():\n",
    "        result = result + multiply(row, income_col, weight_col, stpuma_col)\n",
    "    df = pandas.DataFrame(result)\n",
    "    return df\n",
    "\n",
    "#\n",
    "# If the pickle exists, fetch that. If not, pull the data again and pickle it. The \n",
    "# repo contains the pickle by default, so to rerun this code, delete the pickle or\n",
    "# or rename it\n",
    "#\n",
    "def get_expanded_data(picklename,df,income_col,weight_col,stpuma_col):\n",
    "    if (os.path.isfile(picklename)):\n",
    "        return pandas.read_pickle(picklename,\"gzip\")\n",
    "    else:\n",
    "        df[weight_col] = df[weight_col].astype(int)\n",
    "        \n",
    "        result = explode_weights(df,income_col,weight_col,stpuma_col)\n",
    "        result.to_pickle(picklename,\"gzip\")\n",
    "        return(result)\n",
    "\n",
    "expanded = get_expanded_data(\"expanded.pickle\", to_expand, \"HINCP\", \"WGTP\", \"stpuma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded[expanded[\"stpuma\"]==4200500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Map the weighted data to CBSAs based on PUMA allocation factor\n",
    "\n",
    "*This step is also cached in a file named \"cbsa_dict.pickle\". To do the collection from scratch, delete \"cbsa_dict.pickle\".*\n",
    "\n",
    "We're now going to \"sample\" all the PUMS data we need, from our new weighted dataset, for each CBSA. If a PUMA has an allocation factor of 1, it's entirely contained within our CBSA, so we take all of the rows in our expanded dataset. If it has an allocation factor less than 1, we randomly sample *n* rows from within our expanded dataset where *n* equals the allocation factor (a percentage) times the total rows in the expanded dataset.\n",
    "\n",
    "We're doing this in it's own step and storing the results in a dict of the form {CBSA: [HHINC1, ... , HHINCN]}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "\n",
    "def write_obj(obj,filename):\n",
    "    f = gzip.open(filename,'wb')\n",
    "    pickle.dump(obj,f)\n",
    "    f.close\n",
    "\n",
    "def load_obj(filename):\n",
    "    f=gzip.open(filename,'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [HINCP, stpuma]\n",
      "Index: []\n",
      "4200500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-d8c5ed4adeb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfips_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fips_data_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_coded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpanded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"too_big/fips_dict.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-d8c5ed4adeb4>\u001b[0m in \u001b[0;36mmake_fips_data_dict\u001b[0;34m(df, census, cached_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpumas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PUMAS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpumas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpumas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcensus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfip\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-d8c5ed4adeb4>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(puma, percent, census)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_entries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpuma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_entries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_entries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   3439\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   3440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3441\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0"
     ]
    }
   ],
   "source": [
    "def get_data(puma,percent,census):\n",
    "    all_entries = census[census[\"stpuma\"]==puma]\n",
    "    if percent < 1: \n",
    "        print(all_entries.head())\n",
    "        print(puma)\n",
    "        s = all_entries.sample(frac=percent,random_state=0)\n",
    "        return(s)\n",
    "    return(all_entries)\n",
    "    \n",
    "def make_fips_data_dict(df,census,cached_file):\n",
    "    if (os.path.isfile(cached_file)):\n",
    "        return load_obj(cached_file)\n",
    "    else:\n",
    "        d = {}\n",
    "        for index, row in df.iterrows():\n",
    "            tar = pandas.DataFrame(columns=census.columns)\n",
    "            fip = row[\"GEO.id2\"]\n",
    "            pumas = row[\"PUMAS\"]\n",
    "            for p in pumas:\n",
    "                new = get_data(p,pumas[p],census)\n",
    "                tar = pandas.concat([tar,new])\n",
    "            d[fip] = tar\n",
    "        write_obj(d,cached_file)\n",
    "        return d\n",
    "    \n",
    "fips_data_dict = make_fips_data_dict(top_coded.head(),expanded,\"too_big/fips_dict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
